{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DVT Tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Epicurious recipes \n",
    "* whats cooking - yummy \n",
    "\n",
    "## My notes \n",
    "** nltk (Natural Language Toolkit<br>**\n",
    "Word lemmatization - http://www.nltk.org/index.html\n",
    " \n",
    "It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dir = './'\n",
    "input_dir = '../../_data_tmp/food/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EPI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YUMMY\n",
    "* Whatscookingscript - cousines\n",
    "* 10 most common ingr - counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_json(input_dir + \"train.json\")\n",
    "testdf = pd.read_json( input_dir + \"./test.json\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## whatscookingscript.py\n",
    "\n",
    "GridSearchCV + LogisticRegression scikit-learn\n",
    "\n",
    "classify recipes into their cousines (20): \n",
    "* brazilian\n",
    "* british\n",
    "* cajun_creole\n",
    "* chinese\n",
    "* filipino\n",
    "* french\n",
    "* greek\n",
    "* indian\n",
    "* irish\n",
    "* italian\n",
    "* jamaican\n",
    "* japanese\n",
    "* korean\n",
    "* mexican\n",
    "* moroccan\n",
    "* russian\n",
    "* southern_us\n",
    "* spanish\n",
    "* thai\n",
    "* vietnamese\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import grid_search\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import scikit-learn => wrong syntax\n",
    "\n",
    "# extra\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "traindf['ingredients_clean_string'] = [' , '.join(z).strip() for z in traindf['ingredients']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('wordnet') # neccesart to use WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# -> I need to install nltk\n",
    "traindf['ingredients_string'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) \n",
    "                                           for line in lists]).strip() for lists in traindf['ingredients']]    \n",
    "# traindf['ingredients_string'] = [' '.join(z).strip() for z in traindf['ingredients']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#test dataset\n",
    "\n",
    "testdf['ingredients_clean_string'] = [' , '.join(z).strip() for z in testdf['ingredients']]\n",
    "testdf['ingredients_string'] = [' '.join([WordNetLemmatizer().lemmatize(re.sub('[^A-Za-z]', ' ', line)) \n",
    "                                       for line in lists]).strip() for lists in testdf['ingredients']]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\a604080\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "# Vectorize train\n",
    "corpustr = traindf['ingredients_string']\n",
    "vectorizertr = TfidfVectorizer(stop_words='english',\n",
    "                             ngram_range = ( 1 , 1 ),analyzer=\"word\", \n",
    "                             max_df = .57 , binary=False , token_pattern=r'\\w+' , sublinear_tf=False)\n",
    "tfidftr=vectorizertr.fit_transform(corpustr).todense()\n",
    "# Vectorize test\n",
    "corpusts = testdf['ingredients_string']\n",
    "vectorizerts = TfidfVectorizer(stop_words='english')\n",
    "tfidfts=vectorizertr.transform(corpusts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidftr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictors_tr = tfidftr\n",
    "targets_tr = traindf['cuisine']\n",
    "predictors_ts = tfidfts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training scores:  [0.76509325 0.76329158]\n",
      "Accuracy: 0.7642 (+/- 0.00090) [Logistic Regression]\n"
     ]
    }
   ],
   "source": [
    "# clf2 = LinearSVC(random_state=1, C=0.4, penalty=\"l2\", dual=False)\n",
    "#classifier = LinearSVC(C=0.80, penalty=\"l2\", dual=False)\n",
    "parameters = {'C':[1, 10]}\n",
    "#clf = LinearSVC()\n",
    "clf = LogisticRegression()\n",
    "# clf1 = LogisticRegression(random_state=1, C=7)\n",
    "# nb = BernoulliNB()\n",
    "# rfc = RandomForestClassifier(random_state=1, criterion = 'gini', n_estimators=200)\n",
    "# sgd = SGDClassifier(random_state=1, alpha=0.00001, penalty='l2', n_iter=80)\n",
    "\n",
    "classifier = grid_search.GridSearchCV(clf, parameters)\n",
    "classifier=classifier.fit(predictors_tr,targets_tr)\n",
    "scores = cross_validation.cross_val_score(clf, predictors_tr,targets_tr, cv=2, scoring='accuracy')\n",
    "print(\"training scores:  \" + str(scores) )\n",
    "print(\"Accuracy: %0.4f (+/- %0.5f) [%s]\" % (scores.mean(), scores.std(), \"Logistic Regression\"))\n",
    "\n",
    "\n",
    "predictions=classifier.predict(predictors_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# EnsembleClassifier\n",
    "# eclf = EnsembleClassifier(clfs=[clf1, clf2,nb, rfc, sgd], weights=[3, 3, 1, 2, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "testdf['cuisine'] = predictions\n",
    "# testdf = testdf.sort('id' , ascending=True)\n",
    "# sort was deprecated for DataFrames in favor of needing to user either sort_values or sort_index. \n",
    "testdf = testdf.sort_values(by=['id'],ascending=True)\n",
    "\n",
    "testdf[['id' , 'ingredients_clean_string' , 'cuisine' ]].to_csv(\"test_cuisine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9944"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 most used ingredients... \n",
    "\n",
    "counter... \n",
    "\n",
    "Plot with the 10 most used ingredients.\n",
    "The original recipe, contained in the 'ingredients' column, is cleaned as follow:\n",
    "- to lowecase\n",
    "- replacing symbols\n",
    "- removing digits\n",
    "- stemming the words using the WordNetLemmatizer\n",
    "\n",
    "next: make 'low fat mozzarella' and 'reduced fat mozzarella' => same ingredient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "#cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "# Auxiliar function for cleaning\n",
    "def clean_recipe(recipe):\n",
    "    # To lowercase\n",
    "    recipe = [ str.lower(i) for i in recipe ]\n",
    "\n",
    "    # Remove some special characters\n",
    "    # Individuals replace have a very good performance\n",
    "    # http://stackoverflow.com/a/27086669/670873\n",
    "    def replacing(i):\n",
    "        i = i.replace('&', '').replace('(', '').replace(')','')\n",
    "        i = i.replace('\\'', '').replace('\\\\', '').replace(',','')\n",
    "        i = i.replace('.', '').replace('%', '').replace('/','')\n",
    "        i = i.replace('\"', '')\n",
    "        return i\n",
    "    \n",
    "    # Replacing characters\n",
    "    recipe = [ replacing(i) for i in recipe ]\n",
    "    # Remove digits\n",
    "    recipe = [ i for i in recipe if not i.isdigit() ]\n",
    "    # Stem ingredients\n",
    "    recipe = [ stemmer.lemmatize(i) for i in recipe ]\n",
    "    return recipe\n",
    "\n",
    "# The number of times each ingredient is used is stored in the 'sumbags' dictionary\n",
    "bags_of_words = [ Counter(clean_recipe(recipe)) for recipe in traindf.ingredients ]\n",
    "sumbags = sum(bags_of_words, Counter())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n",
      "[('salt', 18049), ('olive oil', 7972), ('onion', 7972), ('water', 7457), ('garlic', 7380), ('sugar', 6434), ('garlic cloves', 6237), ('butter', 4848), ('ground black pepper', 4785), ('all-purpose flour', 4632)]\n"
     ]
    }
   ],
   "source": [
    "print(type(sumbags))\n",
    "print(sumbags.most_common(10))\n",
    "# dst = pd.DataFrame(sumbags, index=[0]).transpose()[0].sort(ascending=False, inplace=False)[:10]\n",
    "dst = pd.DataFrame(sumbags, index=[0]).transpose()[0].sort_values(ascending=False, inplace=False)[:10]\n",
    "# dst = pd.DataFrame(sumbags, index=[0]).transpose()[0].sort_index(ascending=False, inplace=False)[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "salt                   18049\n",
       "onion                   7972\n",
       "olive oil               7972\n",
       "water                   7457\n",
       "garlic                  7380\n",
       "sugar                   6434\n",
       "garlic cloves           6237\n",
       "butter                  4848\n",
       "ground black pepper     4785\n",
       "all-purpose flour       4632\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, plot the 10 most used ingredients\n",
    "plt.style.use(u'ggplot')\n",
    "fig = dst.plot(kind='barh')\n",
    "fig.invert_yaxis()\n",
    "fig = fig.get_figure()\n",
    "fig.tight_layout()\n",
    "fig.savefig('10_most_used_ingredients.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Cooking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "# import cPickle as pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "# import theano\n",
    "# import theano.tensor as T\n",
    "# import lasagne as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# auxiliary functions for nn\n",
    "\n",
    "def get_param_values(params):\n",
    "    return [p.get_value() for p in params]\n",
    "\n",
    "def set_param_values(params, param_values):\n",
    "    for p, pv in zip(params, param_values):\n",
    "        p.set_value(pv)\n",
    "        \n",
    "def normalize_input(X):\n",
    "    return (X.T / np.sum(X, axis=1)).T\n",
    "\n",
    "def encode_label(l):\n",
    "    v = np.zeros(label_names.size)\n",
    "    print(v)\n",
    "    i = lbl.transform([l])\n",
    "    v[i] = 1.0\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# load and preprocess data\n",
    "\n",
    "input_dir = './'\n",
    "\n",
    "# train\n",
    "with open(os.path.join(input_dir, 'train.json')) as train_f:\n",
    "    train_data = json.loads(train_f.read())\n",
    "\n",
    "X_train = [x['ingredients'] for x in train_data]\n",
    "X_train = [dict(zip(x,np.ones(len(x)))) for x in X_train]\n",
    "\n",
    "vec = DictVectorizer()\n",
    "X_train = vec.fit_transform(X_train).toarray()\n",
    "X_train = normalize_input(X_train)\n",
    "\n",
    "feature_names = np.array(vec.feature_names_)\n",
    "\n",
    "lbl = LabelEncoder()\n",
    "\n",
    "y_train = [y['cuisine'] for y in train_data]\n",
    "y_train = lbl.fit_transform(y_train).astype(np.int32)\n",
    "\n",
    "label_names = lbl.classes_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output\n",
      "[ 6 16  4 ...  8  3 13]\n",
      "input\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "6714\n",
      "39774\n",
      "['brazilian' 'british' 'cajun_creole' 'chinese' 'filipino' 'french'\n",
      " 'greek' 'indian' 'irish' 'italian' 'jamaican' 'japanese' 'korean'\n",
      " 'mexican' 'moroccan' 'russian' 'southern_us' 'spanish' 'thai'\n",
      " 'vietnamese']\n"
     ]
    }
   ],
   "source": [
    "print(\"output\")\n",
    "print(y_train)\n",
    "print(\"input\")\n",
    "print(X_train)\n",
    "print(len(feature_names))\n",
    "print(len(X_train))\n",
    "print(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_in = encode_label('chinese')\n",
    "vec_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# construct nn classifier\n",
    "LEARNING_RATE = 0.01\n",
    "OUTPUT_DIM = label_names.size # = 20\n",
    "\n",
    "BATCH_SIZE = 1 #256 - to create network || 1 for testing\n",
    "NUM_EPOCHS = 1 #100 - to create network || 1 for testing\n",
    "\n",
    "# pad samples\n",
    "n_train = X_train.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sgd(loss, all_parameters, learning_rate):\n",
    "    all_grads = [theano.grad(loss, param) for param in all_parameters]\n",
    "    updates = []\n",
    "\n",
    "    for param_i, grad_i in zip(all_parameters, all_grads):\n",
    "        v = - learning_rate * grad_i\n",
    "\n",
    "        # clip from 0.0 to 1.0\n",
    "        updates.append((param_i, T.clip(param_i + v, 0.0, 1.0)))\n",
    "    return updates\n",
    "\n",
    "def theano_nn():\n",
    "    # theano based neural network\n",
    "    # (2) i/o \n",
    "    X_shared = theano.shared(np.zeros((1, 1,), dtype=theano.config.floatX))\n",
    "    y_shared = theano.shared(np.zeros((1, 1,), dtype=theano.config.floatX))\n",
    "    #y_shared = theano.shared(np.zeros((1,), dtype=theano.config.floatX))\n",
    "    #y_shared_casted = T.cast(y_shared, 'int32')\n",
    "\n",
    "    batch_index = T.lscalar('batch_index')\n",
    "    activation = nn.nonlinearities.rectify\n",
    "\n",
    "    l_in = nn.layers.InputLayer(input_var=X_shared, shape=(BATCH_SIZE, X_train.shape[1]),)\n",
    "    l_hidden0_dropout = nn.layers.DropoutLayer(l_in, p=0.0)\n",
    "\n",
    "    l_hidden1 = nn.layers.DenseLayer( l_hidden0_dropout, num_units=1024,\n",
    "        nonlinearity=activation, W=nn.init.GlorotUniform(),)\n",
    "    l_hidden1_dropout = nn.layers.DropoutLayer(l_hidden1, p=0.5)\n",
    "\n",
    "    l_hidden2 = nn.layers.DenseLayer( l_hidden1_dropout, num_units=1024,\n",
    "        nonlinearity=activation, W=nn.init.GlorotUniform(),)\n",
    "    l_hidden2_dropout = nn.layers.DropoutLayer(l_hidden2, p=0.5)\n",
    "\n",
    "    # classifier\n",
    "    l_out = nn.layers.DenseLayer( l_hidden2_dropout, num_units=OUTPUT_DIM,\n",
    "        nonlinearity=nn.nonlinearities.softmax, W=nn.init.GlorotUniform(),) \n",
    "\n",
    "    # (3) loss, outputs, updates\n",
    "    learning_rate = theano.shared(np.array(LEARNING_RATE, dtype=theano.config.floatX))\n",
    "    all_params = nn.layers.get_all_params(l_out)\n",
    "\n",
    "    # load weights\n",
    "    nn_params = pickle.load(open('nn_params.pkl'))\n",
    "    set_param_values(all_params, nn_params)\n",
    "\n",
    "    #loss_train = T.mean(-T.log( nn.layers.get_output(l_out) )[T.arange(y_shared_casted.shape[0]), y_shared_casted])\n",
    "    #loss_eval = T.mean(-T.log( nn.layers.get_output(l_out, deterministic=True) )[T.arange(y_shared_casted.shape[0]), y_shared_casted])\n",
    "    #loss_train = T.mean( ( nn.layers.get_output(l_out) - y_shared) ** 2.0 )\n",
    "\n",
    "    loss_train = T.mean( ( nn.layers.get_output(l_out) - y_shared) ** 2.0 ) \\\n",
    "        + 0.0001 * T.mean(T.sum(abs(X_shared), axis=1)) \n",
    "\n",
    "    pred = T.argmax( nn.layers.get_output(l_out, deterministic=True), axis=1)\n",
    "    pred_proba = nn.layers.get_output(l_out, deterministic=True)\n",
    "\n",
    "    #updates = nn.updates.nesterov_momentum( loss_train, all_params, learning_rate,)\n",
    "    #updates = nn.updates.sgd( loss_train, [X_shared], learning_rate,)\n",
    "    updates = sgd( loss_train, [X_shared], learning_rate,) # sgd with clip\n",
    "\n",
    "    train = theano.function([], [loss_train], updates=updates,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "print('start training')\n",
    "for e in xrange(NUM_EPOCHS):\n",
    "\n",
    "    # shuffle and pad train sample\n",
    "    idx = np.arange(y_train.size)\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:(idx.shape[0] / BATCH_SIZE * BATCH_SIZE)]\n",
    "\n",
    "    X_shared.set_value(X_train[idx].astype(np.float32))\n",
    "    y_shared.set_value(y_train[idx].astype(np.float32))\n",
    "\n",
    "\n",
    "    train_losses = []\n",
    "\n",
    "    for b in xrange(idx.shape[0] / BATCH_SIZE):\n",
    "        (train_loss,) = train(b)\n",
    "        train_losses.append(train_loss)\n",
    "        (_, p) = test(b)\n",
    "\n",
    "    mean_train_loss = np.mean(train_losses)\n",
    "    print('  epoch: {}, loss: {}'.format(e, mean_train_loss))\n",
    "\n",
    "\n",
    "\n",
    "nn_params = get_param_values(all_params)\n",
    "pickle.dump(nn_params, open('nn_params.pkl', 'w'), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "X_shared.set_value(X_test.astype(np.float32))\n",
    "y_shared.set_value(y_test.astype(np.float32))\n",
    "\n",
    "y_out = []\n",
    "for b in xrange(X_test.shape[0] / BATCH_SIZE):\n",
    "    (_, _y_out) = test(b)\n",
    "    y_out.append(_y_out)\n",
    "\n",
    "y_out = np.vstack(y_out)[:n_test]\n",
    "\n",
    "y_pred = np.argmax(y_out, axis=1)\n",
    "y_pred_label = lbl.inverse_transform(y_pred)\n",
    "\n",
    "df = pd.DataFrame(data=OrderedDict([('id', test_id), ('cuisine', y_pred_label)]))\n",
    "df.to_csv('./submit.csv', index=False)\n",
    "\n",
    "# LB: 0.74728\n",
    "# You can beat the score by using shallow neural network (1 layer NN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cook(input_vec):\n",
    "    y_target = input_vec.reshape(-1, 1).T\n",
    "    X_noise = np.random.uniform(low=0.0, high=1.0,\n",
    "        size=(BATCH_SIZE, X_train.shape[1])) * 0.01\n",
    "    \n",
    "#     print(X_noise)\n",
    "    return\n",
    "    # update white\n",
    "    X_shared.set_value(X_noise.astype(np.float32))\n",
    "    y_shared.set_value(y_target.astype(np.float32))\n",
    "\n",
    "#     for _ in range(20000):\n",
    "#         train()\n",
    "\n",
    "    X_out = X_shared.get_value()\n",
    "    return X_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# main\n",
    "def print_input(vec):\n",
    "    print('- - - - - I N P U T - - - - -')\n",
    "\n",
    "    for i, e in enumerate(vec):\n",
    "        if e > 0.0:\n",
    "            #print('{}: {:0.2f}'.format(label_names[i], e))\n",
    "            print('{:<30} ({:0.4f})'.format(label_names[i], e))\n",
    "\n",
    "def print_out(vec, n_top=10):\n",
    "    print('- - - - - O U T P U T - - - - -')\n",
    "\n",
    "    if vec.ndim > 1:\n",
    "        vec = np.mean(vec, axis=0)\n",
    "    idx = np.argsort(-vec)[:n_top]\n",
    "\n",
    "    for i,j in enumerate(idx):\n",
    "        #print('{:02d} {:<30} ({:0.4f})'.format(i, feature_names[j].encode('utf-8'), vec[j]))\n",
    "        print('{:<30} ({:0.4f})'.format(feature_names[j].encode('utf-8'), vec[j]))\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# chinese cusine devised by the model\n",
    "def chinese():\n",
    "    vec_in = encode_label('chinese')\n",
    "    print_input(vec_in)\n",
    "\n",
    "    vec_out = cook(vec_in)\n",
    "    print_out(vec_out)\n",
    "# chinese()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# british cusine devised by the model\n",
    "def british():\n",
    "    vec_in = encode_label('british')\n",
    "    print_input(vec_in)\n",
    "\n",
    "    vec_out = cook(vec_in)\n",
    "    print_out(vec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# what happenes when combined?\n",
    "def combined():\n",
    "    vec_in = (encode_label('british')  + encode_label('chinese')) * 0.5\n",
    "    print_input(vec_in)\n",
    "\n",
    "    vec_out = cook(vec_in)\n",
    "    print_out(vec_out)\n",
    "\n",
    "    # internatonal cusine?\n",
    "    vec_in = (np.ones(label_names.size))\n",
    "    vec_in = vec_in / np.sum(vec_in)\n",
    "    print_input(vec_in)\n",
    "\n",
    "    vec_out = cook(vec_in)\n",
    "    print_out(vec_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other... tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
